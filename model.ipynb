{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Model:  \n",
    "    \"\"\"\n",
    "    This class represents an AI model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor for Model class.\n",
    "  \n",
    "        Parameters\n",
    "        ----------\n",
    "        self : object\n",
    "            The instance of the object passed by Python.\n",
    "        \"\"\"\n",
    "        # TODO: Replace the following code with your own initialization code.\n",
    "        self.num_classes= 3\n",
    "        self.model = MyCNN(3)\n",
    "\n",
    "    #Filling nan values of X with mean of adjacent pixels\n",
    "    def replace_nan_with_adjacent_mean(self, img):\n",
    "        n_channel,n_row,n_col = img.shape\n",
    "        for channel in range(n_channel):\n",
    "            for row in range(n_row):\n",
    "                for col in range(n_col):\n",
    "                    if np.isnan(img[channel, row, col]):\n",
    "                        adjacent = []\n",
    "                        for i in range(row-1, row+1):\n",
    "                            for j in range(col-1, col+1):\n",
    "                                if 0 <= i < n_row and 0 <= j < n_col and not np.isnan(img[channel, i, j]):\n",
    "                                    adjacent.append(img[channel, i, j])\n",
    "                        if adjacent:\n",
    "                            img[channel, row, col] = np.mean(adjacent)\n",
    "                        else:\n",
    "                            img[channel, row, col] = 0\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model using the input data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Training data.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of the trained model.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        #Clip values from range 0 to 255\n",
    "        X = np.clip(X, 0, 255)\n",
    "        \n",
    "        #Filling nan values of X with mean of adjacent pixels\n",
    "        for i in range(X.shape[0]):\n",
    "            self.replace_nan_with_adjacent_mean(X[i])\n",
    "        \n",
    "        #Flatten X and change to df\n",
    "        flattened_images = X.reshape(X.shape[0], -1)\n",
    "        images_df = pd.DataFrame(flattened_images)\n",
    "        labels_df = pd.DataFrame(y)\n",
    "        \n",
    "        #Remove y rows with nan values\n",
    "        labels_row_with_nan = labels_df.dropna().index\n",
    "        images_df_filtered = images_df.loc[labels_row_with_nan]\n",
    "        labels_df_filtered = labels_df.loc[labels_row_with_nan]\n",
    "        \n",
    "        #Oversampling minority class\n",
    "        majority_n = labels_df_filtered[0].value_counts().max()\n",
    "        data_df = pd.concat([images_df_filtered, labels_df_filtered], axis=1)\n",
    "        majority_index = data_df.iloc[:,768].value_counts().idxmax() \n",
    "        \n",
    "        #Separate majority class and the rest\n",
    "        majority_data = data_df[data_df.iloc[:,768] == majority_index]\n",
    "        minority_data = data_df[data_df.iloc[:,768] != majority_index]\n",
    "\n",
    "        oversampled_minority_data = pd.DataFrame()\n",
    "        #For each minority class, oversample the class with the majority class count\n",
    "        for minority_class in minority_data.iloc[:,768].unique():\n",
    "            minority_class_data = minority_data[minority_data.iloc[:,768] == minority_class]\n",
    "            oversampled_minority_class_data = minority_class_data.sample(n=majority_n, replace=True)\n",
    "            oversampled_minority_data = pd.concat([oversampled_minority_data, oversampled_minority_class_data], axis=0)\n",
    "        \n",
    "        #Merge all the data together\n",
    "        oversampled_data = pd.concat([majority_data, oversampled_minority_data], axis=0)\n",
    "\n",
    "        images_balanced = oversampled_data.iloc[:, :oversampled_data.shape[1]-1]\n",
    "        labels_balanced = oversampled_data.iloc[:, -1]    \n",
    "        \n",
    "        #Change data to numpy\n",
    "        images_balanced = images_balanced.values.reshape(-1, 3, 16, 16)\n",
    "        labels_balanced = labels_balanced.to_numpy()\n",
    "        \n",
    "        \n",
    "        batch_size = 128\n",
    "        #Change dataset to tensor and put in dataloader\n",
    "        image_tensor = torch.tensor(images_balanced, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(labels_balanced, dtype=torch.long)\n",
    "        training_set = TensorDataset(image_tensor, label_tensor)\n",
    "        training_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        #Initialise optimizer and loss function\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "        #Train the model for set no. of epochs\n",
    "        num_epochs = 20  \n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            for inputs, labels in training_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained model to make predictions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Input data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples,)\n",
    "        Predicted target values per element in X.\n",
    "           \n",
    "        \"\"\"\n",
    "        # TODO: Replace the following code with your own prediction code.\n",
    "        \n",
    "        #Clip values that are out of range\n",
    "        X = np.clip(X, 0, 255)\n",
    "        \n",
    "        #Fill in nan values of X\n",
    "        for i in range(X.shape[0]):\n",
    "            self.replace_nan_with_adjacent_mean(X[i])\n",
    "            \n",
    "        #Change X to tensor and run it with the model\n",
    "        images_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        predictions = self.model(images_tensor)\n",
    "        \n",
    "        #Get the class with the highest probability\n",
    "        predictions = torch.argmax(predictions, dim=1).numpy()\n",
    "        \n",
    "        return predictions  \n",
    "\n",
    "    \n",
    "class MyCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(1600, 512)\n",
    "        self.linear2 = nn.Linear(512, 128)\n",
    "        self.linear3 = nn.Linear(128, num_classes)\n",
    "        self.leaky = nn.LeakyReLU()\n",
    "        self.DO = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.leaky(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.leaky(x)\n",
    "        x = self.DO(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.leaky(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.leaky(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open('data.npy', 'rb') as f:\n",
    "    data = np.load(f, allow_pickle=True).item()\n",
    "    X = data['image']\n",
    "    y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(10):\n",
    "    # Split train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "    # Filter test data that contains no labels\n",
    "    # In Coursemology, the test data is guaranteed to have labels\n",
    "    nan_indices = np.argwhere(np.isnan(y_test)).squeeze()\n",
    "    mask = np.ones(y_test.shape, bool)\n",
    "    mask[nan_indices] = False\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = Model()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate model predition\n",
    "    # Learn more: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    print(\"F1 Score (macro): {0:.2f}\".format(f1_score(y_test, y_pred, average='macro'))) # You may encounter errors, you are expected to figure out what's the issue.\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Time taken: {elapsed_time:.6f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
